{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name(s): Ritu Gupta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 6: Q-Learning and Association Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Q-Learning\n",
    "\n",
    "For the first part of this assignment, you'll code an ML agent that uses Q-Learning to learn how to play Snake.\n",
    "\n",
    "<img src=\"snake_game.jpg\" width=\"300\"/>\n",
    "\n",
    "\n",
    "If you are not familiar with the game of Snake, read about how the game is played, and even play it yourself, [here](https://www.coolmathgames.com/blog/how-to-play-snake-mastering-a-classic). \n",
    "\n",
    "Once you understand how to play the game, continue with the assignment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygame\n",
      "  Downloading pygame-2.5.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Downloading pygame-2.5.2-cp311-cp311-macosx_11_0_arm64.whl (12.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pygame\n",
      "Successfully installed pygame-2.5.2\n"
     ]
    }
   ],
   "source": [
    "# you will need pygame installed\n",
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "imported game_loop\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import pickle as pkl\n",
    "import itertools\n",
    "\n",
    "# the environment is provided in game_loop.py\n",
    "from game_loop import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game States\n",
    "For Q-Learning, the agent needs to keep a Q-Table of game states. To keep the number of possible game states manageable, we will define the game state as a combination of:\n",
    "* Horizontal orientation from the food - is the food currently to the left or the right of the snake's head? (-1: left, 0: same, 1: right)\n",
    "* Vertical orientation from the food - is the food currently above or below the snake's head? (-1: below, 0: same, 1: above) \n",
    "* What are in the squares adjacent to the snake's head - are there walls or snake tail in these squares? (1 indicates something is there, 0 indicates it is open)\n",
    "\n",
    "The environment (which is in the provided `game_loop.py` file) will provide this state in the form of two tuples:\n",
    "- The first tuple will contain (relative horizontal orientation to food, relative vertical orientation to food).\n",
    "- The second tuple will contain the squares surrounding the snake's head in the form of (up, down, left, right) where a 1 indicates wall or tail is in that square and 0 indicates that square is open.\n",
    "\n",
    "### Snake Agent\n",
    "Below is a provided class for the SnakeAgent that will learn to play the game, using Q-Learning. \n",
    "\n",
    "Some methods have been provided for you - **do not change these**:\n",
    "* `__init__`: The constructor sets up some constants and sets the learning parameter values. Do not change the learning parameter values for part 1 of this assignment. It also calls init_qtable to initialize the Q-Table. \n",
    "* `init_qtable`: Initializes the Q-Table. The `qtable` is implemented as a dictionary of state -> list of four Q-Values, representing the four action choices of [left, right, up, down].\n",
    "* `save` and `load`: Saves the Q-Values (via pickle) so the agent can be re-loaded later via load.\n",
    "\n",
    "There are two additional methods in the SnakeAgent class **that you will implement**. These will be called by the `game_loop.py` file.\n",
    "* `act`: This method is responsible for selecting an action based on the current state.\n",
    "* `update`: This where the Q-values get updated based on the agent's actions and rewards during gameplay.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Q1. Implement the `act` method in the SnakeAgent class.\n",
    "\n",
    "The `act` method is responsible for selecting an action based on the current state, which is determined by the Snake's position and the relative position of the food. The agent can choose to explore randomly with a probability of epsilon or to exploit the learned Q-values to select the next action.\n",
    "\n",
    "In this method, you should:\n",
    "- Based on the current state, choose an action using an epsilon-greedy strategy\n",
    "  - Explore: Select a random action (0: left, 1: right, 2: up, 3: down)\n",
    "  - Exploit: Select the action with the highest Q-value for the current state\n",
    "- Record the current state and action, as you'll need these later for the update method (use `self.curr_state` and `self.curr_act`)\n",
    "- Return the chosen action as a string (use `self.ACTION_STRING_MAP` in the SnakeAgent class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Implement the `update` method in the SnakeAgent class.\n",
    "\n",
    "The `update` method is a critical part of your SnakeAgent's learning process. It's where you update the Q-values based on the agent's actions and rewards during gameplay. Follow these steps to implement the `update` function:\n",
    "\n",
    "**Rewards**:\n",
    "Positive rewards are given for favorable actions, and negative rewards are given for unfavorable ones.\n",
    "- Check if the game is over using the `game_over` parameter. If it is over, give the agent a reward of -10 to indicate that the agent has lost the game.\n",
    "- If the game is not over, calculate the reward as follows: \n",
    "    - If the snake got closer to the food or if it ate the food, then set the reward to +1. \n",
    "    - If the snake got farther from the food or did not eat the food, then set the reward to -1.\n",
    "You can use `self.curr_dist_from_food` and the incoming parameter from the environment `dist_from_food` to help you determine this.\n",
    "    \n",
    "**Update the Q-value for the state-action pair**:\n",
    "- With the current state and action, update the Q-value for this state-action pair using the Q-learning formula, where `max(Q(s_t_plus_1))` is the maximum Q-value for the next state:\n",
    "$$\n",
    "Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\cdot (R_t + \\gamma \\cdot \\max_a Q(s_{t+1}, a) - Q(s_t, a_t))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeAgent(object):\n",
    "    '''\n",
    "    Do not change this code, except for the 'act' and 'update' methods.\n",
    "    '''\n",
    "    def __init__(self, DIS_WIDTH, DIS_HEIGHT, BLOCK_SIZE):\n",
    "\n",
    "        self.ACTION_STRING_MAP = {0: \"left\", 1: \"right\", 2: \"up\", 3: \"down\"}\n",
    "\n",
    "        # Learning parameters - do not change these for Part 1\n",
    "        self.epsilon = 0.1 # exploration rate\n",
    "        self.alpha = 0.7 # learning rate\n",
    "        self.gamma = 0.5 # discount factor\n",
    "\n",
    "        # Initialize the Q-Table\n",
    "        self.init_qtable()\n",
    "        \n",
    "        # State/Action history\n",
    "        self.curr_state = None\n",
    "        self.curr_act = None\n",
    "        self.curr_dist_from_food = 1000\n",
    "\n",
    "    def init_qtable(self):\n",
    "        '''Initialize the qtable to all 0s'''\n",
    "        self.qtable = {}\n",
    "        REL_Y = [-1, 0, 1] # [below, same, above]\n",
    "        REL_X = [-1, 0, 1] # [left, same, right]\n",
    "        SURROUNDINGS = list(itertools.product(*[[0,1]] * 4)) # [0, 1] possibilities for surrounding block in each direction\n",
    "        for y in REL_X:\n",
    "            for x in REL_Y:\n",
    "                for s in SURROUNDINGS:\n",
    "                    self.qtable[QState((x, y), tuple(s))] = [0, 0, 0, 0] # q values for actions [left, right, up, down]\n",
    "\n",
    "    def load(self, path=\"qtable.pkl\"):\n",
    "        return pkl.load(open(path, \"rb\"))\n",
    "\n",
    "    def save(self, path=\"qtable.pkl\"):\n",
    "        pkl.dump(self.qtable, open(path, \"wb\"))\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    Implement these methods:\n",
    "    '''\n",
    "    def act(self, state):\n",
    "        # TODO: Implement this method\n",
    "        return NotImplementedError\n",
    "    \n",
    "    def update(self, game_over, dist_from_food, new_state):  \n",
    "        # TODO: Implement this method\n",
    "        return NotImplementedError\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Loop\n",
    "The code in this cell will kick off the game, using your SnakeAgent, and will let your agent play for the specified number of eqisodes, keeping track of the scores as your agent learns how to play. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Do not change the code in this cell!\n",
    "(except for setting draw=False if you want to turn off the game visualization)\n",
    "'''\n",
    "\n",
    "pygame.init()\n",
    "\n",
    "BLOCK_SIZE = 10 \n",
    "DIS_WIDTH = 600\n",
    "DIS_HEIGHT = 500\n",
    "FRAMESPEED = 50000\n",
    "NUM_EPISODES = 200\n",
    "\n",
    "#create a SnakeAgent\n",
    "agent = SnakeAgent(DIS_WIDTH, DIS_HEIGHT, BLOCK_SIZE)\n",
    "\n",
    "game_count = 1\n",
    "max_score = 0\n",
    "\n",
    "# loop for specified number of episodes\n",
    "for i in range(NUM_EPISODES):\n",
    "    # use a decaying epsilon rate\n",
    "    if game_count > 100:\n",
    "        agent.epsilon = .01\n",
    "    else:\n",
    "        agent.epsilon = .1\n",
    "        \n",
    "    # Start the game\n",
    "    # (To turn off the game visualizations, you can set draw=False)\n",
    "    score = GameLoop(agent, DIS_WIDTH, DIS_HEIGHT, BLOCK_SIZE, FRAMESPEED, draw=True)\n",
    "    \n",
    "    max_score = max(max_score, score)\n",
    "    print(f\"Games: {game_count}; Score: {score}; Max Score: {max_score}\") # Output results of each game to console to monitor as agent is training\n",
    "    game_count += 1\n",
    "\n",
    "agent.save()\n",
    "print(\"Save Qvals\")\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving and loading the trained agent\n",
    "The code above above saves your trained agent. You can use the cell below to load your trained agent and run it (so you don't have to re-train it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load()\n",
    "pygame.init()\n",
    "GameLoop(agent, DIS_WIDTH, DIS_HEIGHT, BLOCK_SIZE, FRAMESPEED, draw=True)\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit (+5)\n",
    "Improve the Agent's Performance. Can you get a max score above 80 within 300 episodes? \n",
    "Potential ideas: improve the state representation, change the reward function, change the hyperparameters, etc.\n",
    "\n",
    "**Copy the SnakeAgent class from above and paste a copy of it in a new cell (you can make a new cell between the SnakeAgent class and the GameLoop cell). Make any extra credit adjustments to the copy, preserving your original SnakeAgent class so that Q1 and Q2 can be graded.**\n",
    "- Write in a markdown cell what you changed and what your observations were"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Association Analysis\n",
    "\n",
    "Association analysis uses machine learning to extract frequent itemsets and strong association rules from large datasets. In this assignment you'll be implementing one of the most commonly used algorithms for association rule mining - the Apriori algorithm.\n",
    "\n",
    "The dataset (`large_retail.txt`) that we are going to use has been adapted from the [Retail Market Basket Dataset](http://fimi.ua.ac.be/data/retail.pdf). This dataset contains transaction records supplied by a Belgian retail supermarket store. Each line in the file represents a separate transaction with the item ids separated by space. The dataset has 3000 transactions and 99 different item ids.\n",
    "\n",
    "You are also provided with a smaller dataset (`small_retail.txt`) with 9 transactions and 5 different item ids. You can test and debug your implementation on this smaller dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori Algorithm from scratch\n",
    "\n",
    "The Apriori algorithm is a classical algorithm in data mining. It is used for mining frequent itemsets and relevant association rules. In this part, you'll be implementing this algorithm for generating the itemsets that occur enough times to meet the `min_sup` threshold.\n",
    "\n",
    "- Use the `frozenset` data structure in Python, which is similar to `set` in functionality, to represent the itemsets, because `frozenset` is an immutable (hashable) data structure. You can maintain a dictionary that maps from the itemset (a `frozenset`) to its support count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset from file\n",
    "def load_dataset(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        content = f.readlines()\n",
    "        data = [[int(x) for x in line.rstrip().split()] for line in content]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the small_retail dataset\n",
    "small_dataset = load_dataset('small_retail.txt')\n",
    "small_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Implement the function `create_1_itemsets` that takes as input the entire dataset and returns a list of all the candidate 1-itemsets. For example, for `small_retail.txt` it should return:\n",
    "~~~\n",
    "[frozenset({1}),\n",
    " frozenset({2}),\n",
    " frozenset({3}),\n",
    " frozenset({4}),\n",
    " frozenset({5})]\n",
    " ~~~\n",
    "Don't hardcode the item ids, your code should support item ids that are non-sequential. Return the list in sorted (numeric) order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_1_itemsets(dataset):\n",
    "    c1 = []\n",
    "    # your code goes here\n",
    "    return c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Implement function `filter_candidates` that takes as input the candidate itemsets, the dataset, and the minumum support count `min_sup`, and filters out candidates that don't meet the support threshold.\n",
    "\n",
    "Return a list of all the itemsets that meet `min_sup` (as a list of frozensets) and the support count information for all of them (as a `dict`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_candidates(candidates, dataset, min_sup):\n",
    "    retlist = []\n",
    "    support_data = {}\n",
    "    # your code goes here\n",
    "    return retlist, support_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Implement the function `generate_next_itemsets` that takes in frequent itemsets of size `k` and generates candidate itemsets of size `k + 1`.\n",
    "\n",
    "Use either the F(k-1) x F(k-1) or the F(k-1) x F(1) candidate generation method, then **filter the candidate list based on the apriori principle before returning it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_itemsets(freq_sets, freq_1_sets=None):\n",
    "    retlist = []\n",
    "    # your code goes here\n",
    "    return retlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Implement the function `apriori_freq_itemsets` that takes the entire dataset as input and returns all the frequent itemsets that meet `min_sup`. Return a list of all the itemsets that meet `min_sup` (as a list of frozensets) and the support count information for all of them (as a `dict`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_freq_itemsets(dataset, minsup):\n",
    "    retlist = []\n",
    "    support_data = {}\n",
    "    # your code goes here\n",
    "    return retlist, support_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Use the provided `print_itemsets` function to display the frequent itemsets and their supports for the `large_retail.txt` dataset **with a min_support count of 300**.\n",
    "\n",
    "For testing/debugging purposes, the output for the `small_retail.txt` dataset with `min_sup` set to 2 is:\n",
    "~~~~\n",
    "Sup\tFreq Itemset\n",
    "6\t[1]\n",
    "7\t[2]\n",
    "6\t[3]\n",
    "2\t[4]\n",
    "2\t[5]\n",
    "4\t[1, 2]\n",
    "4\t[1, 3]\n",
    "2\t[1, 5]\n",
    "4\t[2, 3]\n",
    "2\t[2, 4]\n",
    "2\t[2, 5]\n",
    "2\t[1, 2, 3]\n",
    "2\t[1, 2, 5]\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THE CODE IN THIS CELL\n",
    "\n",
    "'''\n",
    "If total_tid is not set, support count will be displayed.\n",
    "If total_tid is set to the length of the dataset, support as a percentage/ratio will be displayed.\n",
    "'''\n",
    "def print_itemsets(support_data, total_tid=1):\n",
    "    print ('\\nSup\\tFreq Itemset')\n",
    "    #group itemsets by length\n",
    "    length_dict = {}\n",
    "    \n",
    "    for itemset in support_data:\n",
    "        #sort the itemset\n",
    "        lst = (sorted(list(itemset)))\n",
    "        #add it to dictionary by length\n",
    "        if len(lst) not in length_dict:\n",
    "            length_dict[len(lst)] = []\n",
    "        length_dict[len(lst)].append(lst)\n",
    "\n",
    "    for length in length_dict:\n",
    "        lst = sorted(length_dict[length])\n",
    "        for item in lst:\n",
    "            if total_tid == 1: #support count\n",
    "                print(str(support_data[frozenset(item)]) + '\\t' + str(item))\n",
    "            else: #support ratio\n",
    "                print(str(round(support_data[frozenset(item)] / total_tid, 2)) + '\\t' + str(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Apriori Algorithm using MLXtend\n",
    "\n",
    "#### Installation\n",
    "`MLxtend` is a library that provides functionality for association rule mining. The documentation for this library is available [here](http://rasbt.github.io/mlxtend/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# install the MLxtend library\n",
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "For the third part of this assignment, the data we'll use comes from a bakery called \"The Bread Basket\", located in the historic center of Edinburgh. The dataset contains more than 9000 transactions from the bakery. The file contains the following columns:\n",
    "\n",
    "- Date. Categorical variable that tells us the date of the transactions (YYYY-MM-DD format). The column includes dates from 2016-10-30 to 2017-04-09.\n",
    "\n",
    "- Time. Categorical variable that tells us the time of the transactions (HH:MM:SS format).\n",
    "\n",
    "- Transaction. Quantitative variable that allows us to differentiate the transactions. The rows that share the same value in this field belong to the same transaction.\n",
    "\n",
    "- Item. Categorical variable with the products purchased.\n",
    "\n",
    "In this part, you'll be running the Apriori algorithm from the MLxtend library to generate the itemsets that occur more than the `min_sup` threshold. Based on these frequent itemsets, you'll find association rules that have confidence above the `min_conf` threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset from file\n",
    "def load_dataset(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        content = f.readlines()[1:]\n",
    "    transactions = []\n",
    "    prev_tid = -1\n",
    "    for t in content:\n",
    "        t = t.strip().split(',')[-2:]\n",
    "        tid = t[0]\n",
    "        item = t[1]\n",
    "        if prev_tid != tid:\n",
    "            prev_tid = tid\n",
    "            transactions.append([item])\n",
    "        else:\n",
    "            transactions[-1].append(item)\n",
    "    return transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('BreadBasket_DMS.csv')\n",
    "\n",
    "# ** NOTE: dataset is a 2D list (not a dataframe!) **\n",
    "\n",
    "print(\"Num transactions:\", len(dataset))\n",
    "#Print the first 10 transactions\n",
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Data Cleaning: Many transactions in the dataset include the item \"NONE.\" First, find and remove all the \"NONE\" items from the dataset. There are some transactions that only contain \"NONE,\" so removing \"NONE\" will leave some transactions as empty lists. Remove all the empty lists as well. \n",
    "\n",
    "Once you have removed the NONEs, find the top 10 best-selling items in the bakery. Create a bar chart to display the total number of transactions for each of the top 10 selling items. Sort the bar chart by frequency (the top most sold item first, down to the 10th most sold item)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. \n",
    "\n",
    "a.) Using `mlxtend.preprocessing.TransactionEncoder`, transform `dataset` into an array format suitable for the `mlxtend` library. You will need to call `fit` then `transform`. \n",
    "\n",
    "`TransactionEncoder` learns unique items from the dataset and transforms each transaction into a one-hot encoded boolean numpy array. For example, the resulting encoded dataset will be represented by something like this, where each row is a transaction. If the first transaction contained ['Crepe', 'Jam'], this would correspond to the first row in the encoded table. \n",
    "\n",
    "<img src=\"table.png\">\n",
    "\n",
    "Print the `shape` of the resulting encoded numpy array.\n",
    "\n",
    "b.) `TransactionEncoder` also has a function `inverse_transform` that allows you to tranform one-hot encoded transactions back to the item labels. Try it out on the first 5 transactions and display the items in the first 5 transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. Convert the encoded numpy array from the previous question (part a) into a pandas dataframe. Use the `TransactionEncoder`'s `.columns_` attribute as the column headers. Print the head of the resulting dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q11. Use the `mlxtend.frequent_patterns.apriori` to generate the frequent itemsets with minimum support of 1% (0.01). Display these itemsets along with their support values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q12. Use `mlxtend.frequent_patterns.fpmax` to find and display all of the maximal frequent itemsets along with their support values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q13. Use `mlxtend.frequent_patterns.association_rules` to calculate rules with a confidence level of 0.25 for the frequent itemsets you generated in Q11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q14. An important step in generating a set of association rules is to determine the optimal thresholds for support and confidence. If we set these values too low we will get a lot of rules and most of them will not be useful. \n",
    "\n",
    "Generate the frequent itemsets with minimum support of 0.5% and plot the number of rules generated with respect to the confidence threshold by varying min_conf between 0 and 1 with increments of 0.1. Notice what happens when you increase the confidence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q15. What value would you choose for the minimum confidence threshold based on the previous plot? Explain why as a comment. \n",
    "\n",
    "Display the rules generated for the your chosen value. Take a look at the generated rules. Are they interesting? As a comment, explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

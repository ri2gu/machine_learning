{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your name here: Ritu Gupta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/xmqcjvys3vl2rggmp1scm07r0000gn/T/ipykernel_73935/1875588508.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Enable inline mode for matplotlib so that Jupyter displays graphs\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Standard Headers\n",
    "# You may add additional headers here if needed\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Enable inline mode for matplotlib so that Jupyter displays graphs\n",
    "%matplotlib inline\n",
    "\n",
    "# print your pandas version\n",
    "pd.__version__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Cleaning, Exploration, and Feature Engineering\n",
    "\n",
    "For this part of the assignment you'll be using the 1994 Adult Census Income dataset (`adult.csv`) collected by Ronny Kohavi and Barry Becker. It contains 32,561 records with 14 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Read the dataset from `adult.csv` into a Pandas dataframe. You can find more on reading CSV (Comma Separated Values) data as a Pandas dataframe [here](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html).\n",
    "\n",
    "**Important:**\n",
    "* Notice in the csv file that `?` is used to indicate missing values. We want all of the missing values to be represented by NaNs in the dataframe. Make sure that all of the `?` are converted to `NaN` in your dataframe.\n",
    "* Also, set the `skipinitialspace` parameter to `True`. (This strips the spaces after the commas off of the data which will be necessary for things to work correctly later in the assignment!)\n",
    "* Display the head of your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Start by looking for missing values. How many missing values are there in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Clean up some of the missing values. \n",
    "* Drop all of the records from the dataset that are missing `occupation` or `workclass`.\n",
    "* For records missing values in `native-country`, impute those by using a \"forward fill.\" This will fill in the missing value with the same value as the previous record. \n",
    "* For records missing `age`, leave those as-is for now. We will do a similar-case imputation on them later! \n",
    "\n",
    "Print something to show that there are no longer missing values in `workclass`, `occupation`, or `native-country`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Explore the data:\n",
    "* How is the dataset distributed between people who make <=50K income and those who make >50K income? \n",
    "* How is the dataset distributed between males & females?\n",
    "* Create a plot to show the distribution of the datset by `education`.\n",
    "* Create a plot to show the distribution of the ages in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Create a plot to determine if there are any outliers in the  `capital-gain` column.  \n",
    "Answer as a comment: What values would you consider to be outliers in this column? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Using the `groupby()` method for dataframes [here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html), group the data by  `marital-status`. \n",
    "\n",
    "Display each marital status along with it's mean age, in sorted order from least to greatest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Now fill in those missing age values, using similar case imputation. For each missing age, fill it in with the **median** age of that person's marital-status category. Use **median** rather than mean for this, so that we only use whole numbers for ages. \n",
    "\n",
    "Specifically display the records with row IDs 35, 216, and 1120."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Display everyone who is over 90 years old.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Create a scatter plot between `age` and `hours-per-week` **for only the first 100 datapoints in the dataset.**  \n",
    "Place `age` on the X axis. Color each datapoint according to their income - one color for those that make <=50K, and another color for those that make >50K.\n",
    "\n",
    "Answer as a comment: What observations or insights can you make by looking at this plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. Feature engineering: Using the dictionary provided below, create a new column called `continent` using the existing `native-country` column in the dataframe. You may want to look at the `map()` method for Series [here](https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html). Display the head of the new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please don't change this cell!!\n",
    "continent_dict = {\n",
    "    'Cambodia' : 'Asia',\n",
    "    'Canada' : 'North-America',\n",
    "    'China' : 'Asia',\n",
    "    'Columbia' : 'South-America',\n",
    "    'Cuba' : 'North-America',\n",
    "    'Dominican-Republic' : 'North-America',\n",
    "    'Ecuador' : 'South-America',\n",
    "    'El-Salvador' : 'North-America',\n",
    "    'England' : 'Europe',\n",
    "    'France' : 'Europe',\n",
    "    'Germany' : 'Europe',\n",
    "    'Greece' : 'Europe',\n",
    "    'Guatemala' : 'North-America',\n",
    "    'Haiti' : 'North-America',\n",
    "    'Holand-Netherlands' : 'Europe',\n",
    "    'Honduras' : 'North-America',\n",
    "    'Hong' : 'Asia',\n",
    "    'Hungary' : 'Europe',\n",
    "    'India' : 'Asia',\n",
    "    'Iran' : 'Asia',\n",
    "    'Ireland' : 'Europe',\n",
    "    'Italy' : 'Europe',\n",
    "    'Jamaica' : 'North-America',\n",
    "    'Japan' : 'Asia',\n",
    "    'Laos' : 'Asia',\n",
    "    'Mexico' : 'North-America',\n",
    "    'Nicaragua' : 'North-America',\n",
    "    'Outlying-US(Guam-USVI-etc)' : 'North-America',\n",
    "    'Peru' : 'South-America',\n",
    "    'Philippines' : 'Asia',\n",
    "    'Poland' : 'Europe',\n",
    "    'Portugal' : 'Europe',\n",
    "    'Puerto-Rico' : 'North-America',\n",
    "    'Scotland' : 'Europe',\n",
    "    'South' : 'Asia',\n",
    "    'Taiwan' : 'Asia',\n",
    "    'Thailand' : 'Asia',\n",
    "    'Trinadad&Tobago' : 'South-America',\n",
    "    'United-States' : 'North-America',\n",
    "    'Vietnam' : 'Asia',\n",
    "    'Yugoslavia' : 'Europe',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will want to create a deep copy of the initial dataframe object\n",
    "# so that you can run this cell multiple times without errors.\n",
    "adult_data_copy = adult_data.copy()\n",
    "\n",
    "# add the new column to adult_data_copy\n",
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q11. Plot a bar graph showing the average age of adults from each continent, and show the standard deviations on the same graph.\n",
    "\n",
    "An example bar plot with standard deviation bars:\n",
    "<img src=\"imgs/screen5.png\" width=200>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q12. To reduce the dimensionality of this dataset via feature selection, which attribute or attributes would you eliminate? Explain why as a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Explain here (as a comment):\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Scaling and Dimensionality Reduction\n",
    "\n",
    "For this part you'll be using a subset of the leaf dataset created by professors from University of Porto, Portugal. This dataset consists in a collection of shape and texture features extracted from digital images of leaf specimens originating from a total of 40 different plant species, but for the purpose of this assignment we're only going to consider 4 plant species.\n",
    "\n",
    "<img src=\"imgs/screen4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q13. Read the dataset from `leaf.csv` into a dataframe. Display the head of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q14. Eccentricity of the leaf is a measure of how much the shape of the leaf varies from a perfect circle. Unfortunately the dataset is missing values in the `eccentricity` column. As a comment, explain how you would handle these missing values, then go ahead and handle them (in code). Display the resulting head of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q15. Data scaling: Standardize the features in the `leaf_data` dataset so that all of the features are on the same scale.\n",
    "\n",
    "* Use `sklearn.preprocessing.StandardScaler` to standardize the datasetâ€™s features. Call `.fit_transform` to calculate the scaled data (fit) and replace the existing data with the scaled data (transform).\n",
    "* Note that `fit_transform` returns a numpy array. Put your scaled data back into a DataFrame by doing something like this (subsitute your variable names): `scaled_data = DataFrame(numpy_array_data, columns=leaf_data.columns)`\n",
    "* Display something to show that the data has been scaled correctly (the head of the data frame, *and/or* the mean of each column which should be ~0, *and/or* the standard deviation of each column which should be ~1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q16. Dimensionality reduction using PCA\n",
    "\n",
    "* Use `sklearn.decomposition.PCA` to perform PCA on the **scaled** data.\n",
    "* Set `n_components` to 0.95 and `svd_solver` to 'full', so that the PCA will reduce the number of dimensions to however many dimensions are necessary to retain 95% of the variance. \n",
    "* Use `.fit_transform` to calculate the dimensionality reduction (fit) and to replace the existing data with the transformed data (transform). \n",
    "* Note that `fit_transform` returns a numpy array. Put the transformed data back into a Pandas DataFrame by doing something like this (subsitute your variable names): `pca_data = DataFrame(numpy_array_data)`\n",
    "* Show the head of the resulting DataFrame to make sure the dimensionality has actually been reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
